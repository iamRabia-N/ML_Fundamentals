{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Table of Contents\n1. [Introduction to Naive Bayes](#1)\n   * [1.1. Bayes' Theorem](#1.1)\n   * [1.2. Naive Bayes Assumption](#1.2)\n\n\n2. [Types of Naive Bayes](#2)\n\n\n3. [Steps to Implement Naive Bayes](#3)\n   * [3.1. Data Preprocessing](#3.1)\n   * [3.2. Training](#3.2)\n   * [3.3. Prediction](#3.3)\n   \n\n4. [Performance Evaluation](#4)\n   \n   \n5. [Advantages of Naive Bayes](#5)\n\n\n6. [Limitations of Naive Bayes](#6)\n   \n   \n7. [Example: Text Classification](#7)","metadata":{}},{"cell_type":"markdown","source":"<a id = \"1\"></a>\n# 1. Introduction to Naive Bayes\nNaive Bayes is a simple yet powerful classification algorithm based on Bayes' Theorem. It is used in various areas including text classification, spam filtering, medical diagnosis and more. Despite its simplicity, it often performs remarkably well especially on large datasets with high dimensionality.\n\n<a id = \"1.1\"></a>\n### 1.1. Bayes' Theorem\nBayes' Theorem is the foundation of Naive Bayes. It calculates the probability of a hypothesis given the evidence.\n\n\nP(A|B): Probability of event A given event B.\n\n\nP(B|A): Probability of event B given event A.\n\n\nP(A) and P(B): Probabilities of events A and B respectively.\n\n\n<a id = \"1.2\"></a>\n### 1.1. Naive Bayes Assumption\nNaive Bayes makes a strong assumption of feature independence which means it assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. \n\n\n<a id = \"2\"></a>\n# 2. Types of Naive Bayes\n1. Gaussian Naive Bayes (Assumes that features follow a gaussian distribution)\n2. Multinomial Naive Bayes (Used for discrete counts. It's suitable for text classification where the features are the word counts)\n3. Bernoulli Naive Bayes: (Assumes that features are binary-valued. It's useful when dealing with binary or Boolean features)\n\n\n<a id = \"3\"></a>\n# 3. Steps to Implement Naive Bayes\n\n<a id = \"3.1\"></a>\n### 3.1. Data Preprocessing\n- Clean the data.\n- Split the data into training and testing sets.\n\n<a id = \"3.2\"></a>\n### 3.2. Training\n- Calculate prior probabilities for each class.\n- Calculate likelihood probabilities for each feature given each class.\n\n<a id = \"3.3\"></a>\n### 3.3. Prediction\n- For a given instance, calculate the posterior probability for each class.\n- Assign the instance to the class with the highest posterior probability.\n\n<a id = \"4\"></a>\n# 4. Performance Evaluation\n1. Accuracy (Overall correctness of the classifier)\n2. Precision (Proportion of true positive predictions out of all positive predictions)\n3. Recall (Proportion of true positive predictions out of all actual positives)\n4. F1 Score (Harmonic mean of precision and recall)\n\n<a id = \"5\"></a>\n# 5. Advantages of Naive Bayes\n- Simple and easy to implement.\n- Works well with high-dimensional datasets.\n- Requires a small amount of training data to estimate parameters.\n- Performs well in the presence of irrelevant features.\n\n\n<a id = \"6\"></a>\n# 6. Limitations of Naive Bayes\n- Strong assumption of feature independence which may not hold true in some cases.\n- May perform poorly if a categorical variable has a category in the test data that was not observed in the training data.\n- Requires careful preprocessing of textual data.\n\n\n<a id = \"7\"></a>\n# 7. Example: Text Classification\nLet's consider an example of text classification using Naive Bayes. This code includes sample data for text classification where each document is labeled as either **positive** or **negative**. It then trains a Naive Bayes classifier using this data and makes predictions on new instances. Finally, it evaluates the model's performance using accuracy and classification report metrics.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report\n\ndocuments = [\n    (\"This is a positive review\", \"positive\"),\n    (\"Very good movie\", \"positive\"),\n    (\"Terrible acting\", \"negative\"),\n    (\"Not worth watching\", \"negative\"),\n    (\"Enjoyed the plot\", \"positive\")\n]\n\nX = [doc[0] for doc in documents]\ny = [doc[1] for doc in documents]\n\nvectorizer = CountVectorizer()\nX_counts = vectorizer.fit_transform(X)\n\nclf = MultinomialNB()\nclf.fit(X_counts, y)\n\nnew_instances = [\"Great acting\", \"Awful experience\"]\nnew_instances_counts = vectorizer.transform(new_instances)\npredictions = clf.predict(new_instances_counts)\n\nfor instance, prediction in zip(new_instances, predictions):\n    print(f\"Instance: {instance} --> Prediction: {prediction}\")\n\ny_pred = clf.predict(X_counts)\nprint(\"\\nAccuracy:\", accuracy_score(y, y_pred))\nprint(\"Classification Report:\")\nprint(classification_report(y, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-04-26T18:26:15.190092Z","iopub.execute_input":"2024-04-26T18:26:15.190530Z","iopub.status.idle":"2024-04-26T18:26:15.392206Z","shell.execute_reply.started":"2024-04-26T18:26:15.190497Z","shell.execute_reply":"2024-04-26T18:26:15.390682Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Instance: Great acting --> Prediction: negative\nInstance: Awful experience --> Prediction: positive\n\nAccuracy: 1.0\nClassification Report:\n              precision    recall  f1-score   support\n\n    negative       1.00      1.00      1.00         2\n    positive       1.00      1.00      1.00         3\n\n    accuracy                           1.00         5\n   macro avg       1.00      1.00      1.00         5\nweighted avg       1.00      1.00      1.00         5\n\n","output_type":"stream"}]}]}